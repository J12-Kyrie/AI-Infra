# 大型模型微调综合指南

## 1. 大型模型微调简介

大型语言模型 (LLM) 和大型视觉模型 (LVM) 的参数数量以及训练所需的数据集规模都显著增加。随着这些模型的增长，训练它们的复杂性和成本也随之增加，需要更多的 AI 集群资源和计算能力。微调作为一种关键技术应运而生，它可以在无需从头开始重新训练模型的情况下，使这些强大的预训练模型适应特定的任务或数据集。此过程涉及对预训练模型的权重进行相对较小的调整，或修改其输入和输出，从而降低与完全重新训练相关的复杂性和成本。

![模型参数与数据集规模增长](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930009_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMUludHJ1ZGN0aW9uX3NsaWRlNl9pbWFnZTA.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMDlfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01VbHVkSEoxWkdOMGFXOXVYM05zYVdSbE5sOXBiV0ZuWlRBLnBuZyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=Jg7EzqLex53tHa7x~Kjqw42e5FZ-AsjflugL324Lenl5FXQ2b5c8RXdseiMZnBU4M3uaA6WCDmzcjZ7VWb10vfLOzHTQhSFXg4UQkfG9zqQP-j7lPP4w5w6xyIgW-huIwbhhSewC-yFJ5QEsXIcx1Rl9d3Mnm65LF5r9Lp5o5hPojxMFyqp5kO-1qawLUHJAcQl~tj3axs5~2AhHAc9y8uhe9qMKfjhUDr9fgMczrm0kOdMskr3LlD8JBtMd2w5Nnil7iSd1sEOUXlPstdNZB3eGx~10c2-2eQds2o8Er0m4oMp4cV2A-38ZvH7LDegZgAyWgC0HvkjhwhuceIPAvw__)

![大型模型业务全流程](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930011_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMUludHJ1ZGN0aW9uX3NsaWRlMl9pbWFnZTA.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTFfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01VbHVkSEoxWkdOMGFXOXVYM05zYVdSbE1sOXBiV0ZuWlRBLnBuZyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=PbQP6BrHU2h837z3SUyycxI-1Q3r0NR64U-IJk-kSrD2VZY1~0r2PDXd-6mWx1Et3F7-qF72LnsLLAfQOOMSzmrYo6Ixqr~VNAt4yci-D0buRGxXBGAHESum6slDq2yLNZ8mPW3PGmW2-NvsWc-GvklnE-so1LSjDahLifoi6XX0MpzGgRaPFzp9pNVtuZLbuBSmr0K3VnmGPBPlIGSMUbcQxERZMASX315Fr85gu369jELZFSpV0zC~H2zJotVBju0yQaUMT3YDNNn2QIuIncjb~wMeDfjxF1TZKotIJy15FeCSENIVvb~wMekHtc2xW14jmkOXxSnLMXncnb-hxw__)

本指南将深入探讨大型模型微调的基础知识，探索各种技术、其应用和最佳实践。我们将涵盖：

* **大型模型微调基础：** 介绍什么是微调、大型模型操作的整体工作流程（从集群准备到部署），以及微调有益的常见场景。
* **微调算法：** 了解不同的方法，包括迁移学习、基于适配器的方法（如 Adapters Tuning 和 LoRA），以及其他参数高效技术（如 IA3、P-Tuning、Prefix Tuning 和 Prompt Tuning），以及指令微调。
* **对比分析：** 讨论各种微调策略之间的差异，例如指令微调与提示微调，以及参数高效微调 (PEFT) 与全参数微调。

## 2. 为什么要对大型模型进行微调？

对大型预训练模型进行微调的主要动机在于，希望利用其庞大的知识和能力来处理特定任务或领域，而所需的数据和计算资源通常远少于从头开始训练模型。
以下是微调的一些关键优势和原因：

* **定制化与专业化：** 预训练模型虽然功能强大，但属于通用型模型。微调允许对这些模型进行定制，使其在特定任务（例如，特定行业的情感分析、医学文本摘要）上表现出色，或者理解并生成特定风格或领域的文本（例如，法律文件、创意写作）。
* **提升特定任务性能：** 通过在较小的、特定于任务的数据集上进行训练，微调可以显著提高模型在该特定任务上的性能，尤其是在原始大规模训练语料库中未能很好体现任务细微差别的情况下，其效果优于直接使用通用预训练模型。
* **减少数据需求：** 从头开始训练大型模型需要海量数据集。微调使组织能够利用更小的、针对其需求的精选数据集实现高性能，从而使高级人工智能技术即使在无法获取网络规模数据的情况下也易于获取。
* **资源效率（时间和成本）：** 大型模型的完整训练非常耗时且计算成本高昂，需要对人工智能集群和能源进行大量投资。微调，尤其是参数高效的方法，极大地减少了这些资源需求，使得在预算和基础设施有限的情况下调整最先进的模型成为可能。
* **适应新数据或不断发展的领域：** 领域及其内部使用的语言不断发展。微调提供了一种机制，可以用新数据更新和调整预训练模型，确保它们随着时间的推移保持相关性和准确性，而无需完整的重新训练周期。
* **增强可控性与对齐：** 微调，特别是采用指令微调和基于人类反馈的强化学习 (RLHF) 等技术，可以更好地控制模型行为，使其输出更符合人类偏好、期望格式和安全准则。
* **利用预训练知识（迁移学习）：** 微调是迁移学习的强大应用。它使我们能够将在广泛的预训练阶段学习到的丰富表示和对语言（或视觉）的总体理解迁移到新的相关任务中，从而在性能上取得显著的领先优势。

从本质上讲，微调弥合了通用大型模型与现实应用中特定、细致需求之间的差距，使先进的人工智能更具适应性、可访问性和有效性。

![微调的优势](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930011_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMUludHJ1ZGN0aW9uX3NsaWRlMzJfaW1hZ2Uw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTFfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01VbHVkSEoxWkdOMGFXOXVYM05zYVdSbE16SmZhVzFoWjJVdy5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=a6WMQg9YMoucwPHPjSaDlc2G1Iouk7DcXuZmGs7PDoTMCDdkZmdBluLfAYjArxcVOjoxsX6r0GshPmdeUWqT4w9wQEavVpd-d1Sy26Nbqcdneb-qujJoSiVokemkkLODt-gruUdFNlZ5g1hRk6pIfn3hmKmlhnqMVgI87TbzOqAu52CV2hSOb7sKioNej9lMSBFoYocgKskx8D3tR95hXoCKnC4BrS-tWll1xahUeXzI6GWG4hs346v0x38i9NTBfHi4SplE-Dc2wT6lraTT-Z-S3zMfLWVJPZY-fJqx705nTbkmarQP4nAnBjRFkMyiaRL2FSFphVRypq6HoJXIZQ__)

## 3. 关键概念与术语

理解大型模型微调的概貌需要熟悉几个关键概念和术语。这些是讨论不同技术和所涉流程的基础。

* **预训练 (Pre-training):** 这是初始的、资源密集型阶段，大型模型在此阶段从海量的通用数据集（通常是来自互联网和其他来源的数十亿个词元）中进行训练。其目标是让模型学习通用的语言模式、事实知识和推理能力。此过程通常是自监督的，常使用“下一个词预测”等任务。

* **微调 (Fine-tuning):** 获取一个预训练模型，并在一个较小的、特定的数据集上对其进行进一步训练，以使其适应特定任务或领域的过程。这利用了在预训练期间获得的知识，从而能够以更少的数据和计算量实现专业化。

* **全参数微调 (Full Parameter Fine-Tuning, FPFT):** 在此方法中，预训练模型的所有参数（权重）都会在微调过程中使用特定于任务的数据集进行更新。虽然这通常能带来高性能，但计算成本可能很高，并且需要更多内存，类似于预训练，但规模较小。

* **参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT):** 一系列旨在通过仅更新模型参数的一小部分子集，或通过添加少量新参数来降低微调的计算和内存成本的技术。这使得微调更易于访问和管理。示例包括 LoRA、Adapters、P-Tuning 等。

* **迁移学习 (Transfer Learning):** 微调的基本原理。这是一种机器学习方法，其中为某个任务开发的模型被重用为第二个任务上模型的起点。预训练的 LLM 是迁移学习的典型示例，其中从通用语言理解任务中获得的知识被迁移到更具体的下游任务中。

* **监督微调 (Supervised Fine-Tuning, SFT):** 一种使用标记数据集的微调过程，其中每个输入（例如，问题、提示）都与期望的输出（例如，答案、故事）配对。模型学习生成与标记示例类似的输出。指令微调是 SFT 的一种形式。

* **指令微调 (Instruction Tuning, IT):** 一种特定类型的 SFT，其中模型在（指令，输出）对的数据集上进行训练。目标是教会模型有效地理解和遵循人类指令，从而提高其基于提示执行各种任务的能力。

* **情境学习 (In-Context Learning, ICL):** 一种技术，模型通过在推理时直接在其输入提示中提供一些示例（演示）来学习执行任务，而无需对模型权重进行任何更新。模型使用这些示例作为上下文来理解期望的任务和输出格式。

* **对齐 (Alignment):** 确保模型的行为和输出与人类偏好、道德准则和期望目标保持一致的过程。这通常涉及诸如基于人类反馈的强化学习 (RLHF) 之类的技术，以使模型更有用、无害和诚实。

* **基于人类反馈的强化学习 (Reinforcement Learning from Human Feedback, RLHF):** 用于使 LLM 与人类偏好对齐的多步骤过程。通常包括：
  
  1. **监督微调 (SFT):** 在一组高质量的提示-响应对上进行初始微调。
  2. **奖励模型训练:** 通过学习人类排序的模型输出来训练一个单独的模型（奖励模型）来预测人类偏好。
  3. **强化学习优化:** 使用强化学习算法（如近端策略优化 - PPO）进一步微调 LLM，使用奖励模型引导 LLM 生成人类更偏好的响应。

* **提示微调 (Prompt Tuning):** 一种 PEFT 方法，其中将一个小的、可学习的“提示”（连续嵌入序列）前置于输入，并且在微调期间仅更新这些提示嵌入，而基础 LLM 保持冻结。

* **前缀微调 (Prefix Tuning):** 类似于提示微调，但是可学习的前缀嵌入被添加到 Transformer 模型的每一层，而不仅仅是输入层。LLM 参数保持冻结。

* **低秩适应 (Low-Rank Adaptation, LoRA):** 一种 PEFT 技术，它将可训练的低秩矩阵注入预训练模型的各层中。在微调期间，仅更新这些较小的矩阵，从而显著减少可训练参数的数量，同时通常能实现与全参数微调相当的性能。

理解这些术语对于理解关于不同微调方法及其含义的讨论至关重要。

![微调技术分类](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930011_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMUludHJ1ZGN0aW9uX3NsaWRlMTFfaW1hZ2Uw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTFfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01VbHVkSEoxWkdOMGFXOXVYM05zYVdSbE1URmZhVzFoWjJVdy5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=ncm~OS6AdgjvKixPKb9Ds1VJhcIJq44dTc2p62RF1XYgs9zJuUSTgJQkJEU~wu6HObxYQlenen6e6nlHxZTxnRpXO5hWIpw0m4mzm9V-knXDAiSPK8AGSerSNSkWeLbsrAjRZTerMVOny3iL5e7nr5Y4z9nTi090LEtF6xeW6ckP8dzK4WJtyB~QtmTuhkmyf4h617J7~31tmC4nq7YiasIdYIgpLNXGIpLwM4aORJAbkfhEx-Qjo8WcGOLxc7SMz8DNb3f5s~fndhiAhq5Oy1qGByQL1wkGkzPbN92szrDTZiNZlojd1NW7c2giB3Z8QdaSJdNA0Jx78zNnc9vXoA__)
## 3. 大模型的微调基础
### 3.1微调的基本介绍
### 3.2微调使用场景
### 3.3大模型微调流程

## 4. 不同的微调技术,微调放在哪一层

大型模型的微调可以通过多种方式进行，大致可根据参数更新的范围和所采用的具体机制进行分类。技术的选择通常取决于可用资源、特定任务以及性能与效率之间的权衡。

### 4.1. 全参数微调 (FPFT)

如关键概念中所述，全参数微调涉及使用特定于任务的数据集更新预训练模型的所有权重。虽然这种方法通过允许模型全面适应可以实现高性能，但它在计算、内存和时间方面是资源最密集的方法。它需要仔细管理学习率和批量大小等训练参数，以避免灾难性遗忘（模型丢失其预训练知识）或对较小的微调数据集过拟合。

### 4.2. 参数高效微调 (PEFT) 方法

PEFT 方法旨在显著减少可训练参数的数量，从而在不大幅降低性能的情况下，使微调更易于访问和高效。已经出现了几种 PEFT 技术，每种技术都有其独特的修改或增强预训练模型的方法。

#### 4.2.1. 作为基础的迁移学习

迁移学习是微调的核心思想。它涉及重用从源任务（预训练任务）中学习到的知识（例如，学习到的特征、权重），以改进目标任务（微调任务）的学习。虽然微调本身是迁移学习的一种形式，但该术语的用途也更为广泛。在某些情况下，迁移学习可能涉及仅获取预训练模型的一部分（例如，Transformer 的编码器）并将其附加到一个新的、随机初始化的部分（例如，新的解码器或分类头），然后在目标任务上进行训练。这与全参数微调不同，后者通常保留整个模型架构，并且其所有（或大部分）参数都会更新。例如，在计算机视觉 (CV) 中，卷积神经网络 (CNN) 通常用于迁移学习。CNN 的浅层学习边缘和曲线等通用特征，而深层则学习更复杂、特定于任务的特征。人们可能会冻结预训练 CNN 的早期层（特征提取器），仅针对新的分类任务训练后期的全连接层，尤其是在数据有限的情况下。

![可视化CNN层：浅层学习通用特征，深层学习更复杂特征。](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMlRyYW5zZmVyX3NsaWRlOV9pbWFnZTA.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01sUnlZVzV6Wm1WeVgzTnNhV1JsT1Y5cGJXRm5aVEEucG5nIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzY3MjI1NjAwfX19XX0_&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=s8yxIiXT3RpSJqEQD0vPAIeWaHzoDOOwfdBN1dqhvRc~N-D-VS0ZphVakDh6g7A88lzwT-0-qq-3VnoVfvvuyWkQ6babYBqPDfaybOwW6qIjqwrWVLaZbfDzsGl9mxxlr0uUqoxpiGpPOOj1u4xrkOAeD3E955MenZuh82wndu7wsSc6JWpSRTqAtnBr0p5aDEcL6TcsNX-Kum1YaTiZsavrFgcLVmwFrhk62mzu9OOHf~AfhL8zPo9iS8KhWIZP6w5QRcMbbShj22uCeja~lRJx1lX-zJ88urZ~zNRYJUTBgYE8cwe0IKo6DGJTt1dFhKdw4Jlm6TtnqRDkcJjAtQ__)

这与全参数微调不同。

#### 4.2.2. Adapter 微调 (Adapters)

由 Houlsby 等人 (Google) 在论文《Parameter-Efficient Transfer Learning for NLP》中提出的 Adapter 微调是早期著名的 PEFT 技术之一。其核心思想是在预训练 Transformer 模型的每一层中插入称为“适配器 (adapters)”的小型可训练神经网络模块。在微调期间，Transformer 模型的原始权重保持冻结，仅更新这些新添加的适配器模块的参数。

**Adapter 模块结构：**

* 适配器通常由一个下投影层（降低维度）、一个非线性激活函数和一个上投影层（恢复原始维度）组成。
* 通常在适配器周围包含一个跳跃连接（残差连接），如果适配器的贡献没有益处，则允许模型学习恒等函数。

![Adapter 模块结构](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMlRyYW5zZmVyX3NsaWRlMTZfaW1hZ2Uw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01sUnlZVzV6Wm1WeVgzTnNhV1JsTVRaZmFXMWhaMlV3LnBuZyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=qfgcj4uadW4Qqp6u6fiUgq5ZXeX3q-ZXvL4pg-KVHfJwd4t5hO6U3XWzMJiSN2AGLpY~-ctk04xb9Dcfx91h74IuqJkrH~sYLYKMQypZ1enaKlj2w3KGgXUFNjSOm81YXmCAuJfatLXFsAUuMwZOG7rkdtiXODk10~fj-aqhcsedUIBIOQ9ogFacIFjFFQZ-AzeVjk-Wtm0IkdIr~vLf32LCUkbAEWz~HCQ6viCEUHQ9Mm7YmZoXLhGscEGK-IPqv2Ksra~0EeTmpu~s2vpZnMDKDCSSd7L4Axz5mJCkFzlAAwnnMicFb7s5VhT0HNmW5UNzL33oWRbp6IYKCttf~w__)

这种设计确保只训练少量附加参数（例如，如论文中所述，约为原始模型参数的 3.6%），从而显著提高效率。实验表明，适配器微调可以在 GLUE 等基准测试中实现接近全参数微调的性能，而可训练参数要少得多，并且如果适配器被合并或得到有效处理，则不会显著增加推理延迟。

#### 4.2.3. 低秩适应 (LoRA)

LoRA 由 Hu 等人 (Microsoft) 在论文《LoRA: Low-Rank Adaptation of Large Language Models》中提出，它解决了这样一个观察结果：虽然预训练语言模型具有许多参数，但在适应特定任务期间权重的变化通常具有较低的“内在秩”。这意味着权重更新可以由低秩矩阵表示或近似。

**LoRA 机制：**

* LoRA 不是直接更新层的原始权重矩阵 `W`，而是引入两个较小的低秩矩阵 `A` 和 `B`（其中秩 `r` 远小于 `W` 的维度）。
* 权重的更新由乘积 `BA`（或 `AB`）表示。因此，修改后的前向传播可以表示为：

```
h = Wx + BAx 
```

或者，如果合并：

```
h = (W + BA)x
```

* 在微调期间，原始权重 `W` 被冻结，仅训练 `A` 和 `B` 的参数。
* 对于推理，乘积 `BA` 可以与 `W` 合并（即 `W_adapted = W + BA`），因此与原始模型相比不会引入额外的推理延迟。

![LoRA 机制图 1](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMlRyYW5zZmVyX3NsaWRlMjBfaW1hZ2Uw.gif?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01sUnlZVzV6Wm1WeVgzTnNhV1JsTWpCZmFXMWhaMlV3LmdpZiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=I3G3IARKrwN8VBo0Y0KitXVYnAYtl-uhOeXbfYdL0ij832BHUMGIyAVqBKfr-q2DzLDhBTZFWA5A78NNygfawqKpLmehgNRU1-b3zyQELqcBdibBnJ3sCntdL4f6Hf4OPK7WZTPB9-nYoLrQXPU2IELigcQyFNSI2eg~7R0S32st9Cr5DasU~k9AbSWOFk8DJoD5WLLyShEXCMwsqPSxMxkmQvZf6arHPsNBdI1SkwaIwIKUqr2AtqVSl2apZ5Eof~JzOxfHoon3lgicHNQnVosFI5GTaklSxWsJoyrr0TbXju2rS-0eUnf0xmtZmlDW99nX2p7n1CI5CU937GA0KA__)
![LoRA 机制图 2](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMlRyYW5zZmVyX3NsaWRlMjFfaW1hZ2Uw.gif?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01sUnlZVzV6Wm1WeVgzTnNhV1JsTWpGZmFXMWhaMlV3LmdpZiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=HQn2H8oB8XEWsioErigZyyX~HNZyuN73MkYfVkM-sAifbn1BPcgZpsUl4x13lw5TnrJ9s8~em6RD1d65q1q73hEUmuOd2QPiqxoTmU5EICh8MXO5SDIaDWD0WnFqIVN0CM7vAD-4YXuxv0lMSx2S2985i99CyvSN1~8JTpwpOyBpuEUsyDDG4I1FflURdz-skifxEcre-lBfVAak83X8fff5ZlQfTURN5487VQaSa-HBVPYLNld4668zP2onmrZxdygy4bFEOUXitmrc0JAXx6hH9RnOxUyTo1TQN4PR3tKPeMQaERQ6kjt1ak6RYxsv2o798RERgmZ8T593heA7Bg__)

**LoRA 的优势：**

* **显著减少参数：** 仅训练 `A` 和 `B`，从而大大减少了可训练参数的数量。
* **无推理延迟：** 学习到的更改可以合并回原始模型权重中。
* **性能有效：** LoRA 已被证明在各种任务上都能达到与全参数微调相当甚至超过全参数微调的性能。
* **任务切换：** 可以为不同的任务训练多个 LoRA 适配器（A、B 对），并且可以轻松地换出它们，而无需修改基础模型，从而实现高效的多任务部署。

LoRA 变得特别流行，不仅用于 LLM，还用于其他领域，例如使用 Stable Diffusion 等模型进行图像生成。它允许用户通过 LoRA 进行微调来创建自定义样式或概念，然后将这些 LoRA 权重组合或应用于基础模型。

![LoRA 在 Stable Diffusion 中的应用示例 1](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMlRyYW5zZmVyX3NsaWRlMjZfaW1hZ2Uw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01sUnlZVzV6Wm1WeVgzTnNhV1JsTWpaZmFXMWhaMlV3LnBuZyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=TyM1IXm8VlSq7FhNnveWn7b1oqyOvn~IUvCH8QPJWcSRkk-Ia~TwEaL003O8uAsNIhKNBqnE8MgPmSQraNTafchvpTocN0NLvqDjTAMRfBL4JtNrcuA1Y8CTk-E9jwN7XuXATv2CBPcghZnGs06OSNjjTjLUOPXuB8AEtAv36l0Aojsv2kS5qNZZo7u4~D4gE2oU1Doq0ST6acFP9zEl97fVqk~Ty74a1h9CZjd~XHluSBFV6vUhg~hGBVpZBK~8pWfYdn~uRCYSztHfb1bCtfhAPp3vaGYVmMg6ALnpGH6dwGRXB24JNpSjsWs4MSk~9EyNO09Xo8T~gYsf3se35A__)
![LoRA 在 Stable Diffusion 中的应用示例 2](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMlRyYW5zZmVyX3NsaWRlMjdfaW1hZ2Uw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01sUnlZVzV6Wm1WeVgzTnNhV1JsTWpkZmFXMWhaMlV3LnBuZyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=HGYxMblz-musjf33Bs8zNR9EnMpZPEdTptMMUGYpwVryI69DCOmK9RCU~XOlbo1VNvtsL1FJYnaqdFun3264qrwAWfLab5BOItQikBuiFA61r0q7dc9GXs7YFat3QbYLE9Y8mVxfEFwdOEv52rB3QZjvVI5~C8jE0Qv0MyfDpRlSAOzYSUFQTHR7R-Bk9mrmUmQGoRW9s9rIaSEKGqgT0xuTYEqFqQXKygctahl767eahplomP3T5B-oIAyXX5qQTiSWp9p26ZfQA1ikOVq2Sm05Y7T5nSXM08Dqw6aT0NnswdOz3RwcgDxIhtrTKnc28NsD5IBjfUjfosWBjxYSZw__)
![LoRA 架构概览](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMlRyYW5zZmVyX3NsaWRlMjJfaW1hZ2Uw.gif?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01sUnlZVzV6Wm1WeVgzTnNhV1JsTWpKZmFXMWhaMlV3LmdpZiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=MvXCRVsxRPtIAwcZxee4kWuZFJVDBEdwzR~VymLkbyis~tTKNjWyvU2LBI5nHiMbU33hK4GRvyb6UklIO0I7FWv-d2~VosSEL4HL~B-spV96k0~wCaIEUcr8S2gVU6GVS91UsOd0QPIpYBFh3k8WBkDARCf0iCkWrJu-ecJL7Ga8zsAX8Urba4auHVuaRzaso59L4XY-wL2QD~Jw9Wi3jlhONecy28pzxVqDdtKyoLoNChOXG7LtGi9KiIstXSgVFqjqoRjzWzCgTbDbxspU5hkuO9XQjVriMLxTZREI22-C6y6wVujQgvT~M1KDaLBdJv7x-4apjt8WmCOVRDHBOw__)

#### 4.2.4. (IA)^3 - 通过抑制和放大内部激活注入适配器

(IA)^3，或 IA3，是另一种 PEFT 技术，旨在比 LoRA 等方法更具参数效率。 (IA)^3 不是添加适配器层或低秩矩阵，而是引入学习向量来重新缩放 Transformer 模型中的内部激活。

**IA3 机制：**

* (IA)^3 为每个注意力块或 MLP 块学习三个向量：`l_k`、`l_v` 和 `l_ff`。
* 这些学习到的向量与注意力机制中的键 (`k`)、值 (`v`) 激活以及前馈网络 (FFN) 块中第一个线性层之后的激活进行逐元素相乘。
* 在微调期间，仅训练这些重新缩放向量，而原始模型权重保持冻结。

![IA3 机制概览](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wM1BUdW5pbmdfc2xpZGU1X2ltYWdlMA.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d00xQlVkVzVwYm1kZmMyeHBaR1UxWDJsdFlXZGxNQS5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=IBnP-Ae1rB~ZuKeAJNfTwm2xVrBkS7yWB4jdHNCmhrHtDXvo6IvuERR9ZqAU2E9SSJuTRLQpIpL136-WAw~i91w1F3b~M-diX037FpE2VCA-WRSnX4PwgkfDqiYAzt6owEE0i5K5u~XsRcmsUxwn5CZupsWV6Q9s2aHeW5kKS7HJNfcs~mpvViE0dEeQ67yJHjU2yOTlX7PavBj0pMwqSJ1JkGhuOScvChaWITDLOuKuPoPWv9AG90rjqYqQ-OeZW3WNMa6In44LR6PIp2f2lT-20yMSHrF0J8cZ2AxOAxp~IFh00f5P2bUnysLdcCsVwrc8Zs3wu7l4befnk3ZBQQ__)
![IA3 正向激励与负向抑制](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wM1BUdW5pbmdfc2xpZGU3X2ltYWdlMA.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d00xQlVkVzVwYm1kZmMyeHBaR1UzWDJsdFlXZGxNQS5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=wHlv23VKHN7e24w4S0X-vicuUW1~8xM9Tlx-ZPc4pbtXnqrQvJVFI8G8vBrHBiMPEc9Uyr-OsurCVy1EwQsSnK-x-Ytbk-oRAIJ1Fz5D3mN19KWla6kah~7nmNElcg5WV58gmuu-95zb-WQ~OZaCzdQCeJfu61--Up4E~5wn2pYyiOP3Yim9SWK9qNdJTBI0sZmbuNBpX4RoUuedx9iaIkk9K3NWaknzE5DAk25V-74aGeJEcZvKQrPSPU-4RcS1pPGvI2zgVaibJNsjweKR1b6KeruNCaemEEFgOir-96moH2M2kEKUa~SplfxHqb4h8lSMtNReBbUnvbfOhZBT-w__)
![IA3 重新缩放向量](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wM1BUdW5pbmdfc2xpZGU4X2ltYWdlMA.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d00xQlVkVzVwYm1kZmMyeHBaR1U0WDJsdFlXZGxNQS5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=R-GFVWJ0xw1-Jn-aoIuh2JiAB63XvYb0q49aDHFpYqxjggBiP2WmjeJCwigy-WOAZzC5LSk~llw8Lv5euarozL32zk7-88gKCFZgWdus7752RcdhdmQLeJAyf5ltDrHG6fbIK5zLeqTLyZN-BTZY9dF8Z8HL1qGDZ3m5IBVlvqcMYJfoKnPvcpdPObU2k1UQ5GDILFdRj0fG-R~Rd~Jt1PcOe~aGYGYlcyQzvOTL-9whn9tAbwPKLJRRiEa0N2ZwMkDwNKJ1yD5YSLE79Mx9ZhlES3F5heDHJ3b7qL8MxlANj1S38pdhE0LlMQOtol~rcxbRf4b37WE8ip9ZBciFNg__)

**Advantages of (IA)^3:**

* **极高的参数效率：** (IA)^3 可以将可训练参数的数量减少到原始模型的极小一部分（例如，约占预训练模型参数的 0.01%，甚至可能低于 LoRA）。
* **有效性：** 尽管 (IA)^3 简单且参数数量最少，但它已在各种任务上表现出强大的性能。
* 其核心思想是通过重新缩放激活来放大或抑制预训练模型学习到的现有特征，而不是学习全新的转换。

#### 4.2.5. P-Tuning

#### 4.2.6. 前缀微调 (Prefix Tuning)

前缀微调由 Li 和 Liang (Stanford) 在《Prefix-Tuning: Optimizing Continuous Prompts for Generation》中提出，是一种 PEFT 方法，专注于在输入序列前添加一个小的、可学习的“前缀”。

**前缀微调机制：**

* 将一个连续的、特定于任务的虚拟词元序列（前缀）前置于输入嵌入。
* 在微调期间，仅更新此前缀的参数，而主 Transformer 模型的参数保持冻结。
* 与离散提示（由人工设计且固定）不同，这些前缀向量是可学习的，并通过反向传播进行优化。
* 前缀向量通常添加到 Transformer 的每一层的键和值状态中，从而影响整个模型的注意力机制。

![前缀微调机制](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wM1BUdW5pbmdfc2xpZGUxMF9pbWFnZTA.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d00xQlVkVzVwYm1kZmMyeHBaR1V4TUY5cGJXRm5aVEEucG5nIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzY3MjI1NjAwfX19XX0_&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=BlNDVQMqgusPOv7wPYFlu0~pSsymkuNp0xxTfSvG75Fd16zvTZzWdg150VXg00c5y0JMbrcaf~Sl~9BpxfIAcm-6k8y0fZ15hpQDyd3lShBSqkLeawpLFBWRAb5W3Qmk7vaQbJ31sjnXhiJmBSutUYWvRqv7fOoeSye3zr~xlp2jtUv-IOEJcXUQUErxWEBaMDNZcC52f2E-7pdsfkpfih7y7hOgSxWn2M017dVbtA0a-S5WOOK-4EmtwmZSKdefinlOKBPPDOuwVHGuyEz-fvB~al0d658n7xaItjuY7BgS7f5noyGttOYm~HSkaPbFA8TUGXCiMkmmsh0BNaNIhA__)

**前缀微调的优势：**

* **参数效率：** 仅训练前缀参数。
* **表达能力：** 通过影响所有层，前缀可以比仅修改输入层的方法更有效地调节模型的行为。
* **不更改基础模型：** 预训练模型的架构和权重保持不变。
#### 4.2.7. Prompt Tuning 提示微调
#### 4.2.8. Prefix Tuning 指令微调
#### 4.2.9. 提示微调 (Prompt Tuning) 与 P-Tuning

提示微调与 P-Tuning 密切相关，有时可互换使用，它们也涉及学习连续的提示嵌入。与前缀微调不同，这些方法通常只在输入层修改嵌入，而不是在每个 Transformer 层都修改。

* **提示微调 (Lester et al., 2021):** 专注于为每个下游任务学习一个特定于任务的“软提示”（一系列连续嵌入），该提示前置于输入文本。LLM 的其余部分保持冻结。这种方法非常节省参数，因为只有提示嵌入被更新。

* **P-Tuning (Liu et al., 2021):** 与提示微调类似，P-Tuning 也学习连续的提示嵌入。一个关键的区别在于 P-Tuning 通常使用一个小型 LSTM 或 MLP（称为提示编码器）来生成这些连续提示嵌入，而不是直接优化它们。这可以提供更好的稳定性和性能，尤其是在较小的模型或数据较少的情况下。

![P-Tuning 机制](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wM1BUdW5pbmdfc2xpZGUxMl9pbWFnZTA.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d00xQlVkVzVwYm1kZmMyeHBaR1V4TWw5cGJXRm5aVEEucG5nIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzY3MjI1NjAwfX19XX0_&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=tzVlDm5Ae7n8E3qKUR1Xnnu~kJxKmza7P4VWm1b1NmSaxNuAIGj-GzcU3dAnvbv87okz3ASdV42Dg6-Y66ZhbSozUgmP3rSEEAhi1uSrvpnHtY~CHoiYwEHCtJOkR6Nyc0CCISFs7RyFvN8XksVEmpogbl2hRp19QDJQMZ3epbX6jreHGjS-M5Z02Bv043JZWC66LvZBv7rFOSrneC~2ujizpbjsIoaFMDDY1ncUi9xmQJna7TuObnIs0R7I56gR~m-ebxOpDYcZFqedzhHE3-ISrtQr7L~cMDkf~4J4m-cX8texi0MnmNtkL2xiMzp8KbSf8rRYRTtX2aPur5z9Rg__)

**提示/P-Tuning 的优势：**

* **极高的参数效率：** 通常比 LoRA 或 Adapter 需要更少的可训练参数。
* **易于实现：** 概念相对简单。
* **任务特定提示：** 可以为许多任务学习单独的提示，并且可以在推理时轻松切换。

这些 PEFT 方法为有效调整大型模型提供了一系列方法。它们之间的选择可能取决于 LLM 的具体架构、下游任务的性质、可用的计算资源以及性能和参数效率之间的期望权衡。

## 5. 微调分步指南（典型 LLM 训练工作流程）

![经典 LLM 训练工作流程](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMUludHJ1ZGN0aW9uX3NsaWRlMjNfaW1hZ2Uw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01VbHVkSEoxWkdOMGFXOXVYM05zYVdSbE1qTmZhVzFoWjJVdy5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=cLzuKcDQyFxDSZRt6vMgBpDprBb6Nwpt38F-BbkQ00JdhMI6VdW4siScKvIprxg~BVf7Zk9qit55oSLLVa41pnYPuJhacKh~8LCYvuXBqsIDbQmq0YQGTUjZ48S~FlUe9ZG5ToEeKzaosvxd-51BUwME-NaOcg06diumaj3raZNx-~NbJugIaMbuZ6OQrLC0AB9-RJU9Jc1KiFKtVO8f2vUmrc1uK~3PQBPCEwyRurs2UIw9t248G6AEireooR1oP5O3D0aBX5hKJx6QLh4a28ZPRRewONj2xplxx8qLZdIhRQGRVB4~hzUEWEgC1FRAVTJSlhCrs~iEAWNhtMX7CQ__)

虽然具体的微调程序会因所选技术（例如 LoRA、提示微调）和任务而异，但大型语言模型的经典训练工作流程（通常包括微调阶段）通常遵循以下关键阶段。了解这个更广泛的工作流程有助于理解微调在其中的位置。

### 5.1. 阶段 1：预训练

这是基础阶段，大型模型从海量的未标记文本数据中学习通用的语言理解能力。目标是使模型能够捕捉语言的统计模式、语法结构和一定程度的常识知识。预训练通常是自监督的，例如，模型被训练来预测句子中的下一个词或填补文本中的缺失部分。

### 5.2. 阶段 2：有监督微调 (SFT) / 指令微调

在预训练之后，模型通常会进行有监督微调 (SFT)，通常采用指令微调的形式。在此阶段，模型会在一个由高质量的（指令，输出）对组成的数据集上进行训练。这有助于模型学习遵循人类指令并以期望的格式生成响应。例如，对于问答任务，指令可能是问题，输出是正确的答案。

### 5.3. 阶段 3：奖励建模 (RM)

为了进一步使模型的行为与人类偏好对齐，通常会训练一个奖励模型。此阶段涉及：

1. 从 SFT 模型中对给定的提示采样多个输出。
2. 让人类评估员对这些输出进行排名，从最好到最差。
3. 使用这些排名数据来训练一个单独的模型（奖励模型），该模型学习预测人类对给定模型输出的偏好分数。奖励模型的目标是为“好”的响应分配高分，为“坏”的响应分配低分。

![奖励模型训练流程](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMUludHJ1ZGN0aW9uX3NsaWRlMjZfaW1hZ2Uw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01VbHVkSEoxWkdOMGFXOXVYM05zYVdSbE1qWmZhVzFoWjJVdy5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=eKQvj8sUCUOIw8fd99u1HolnfLOG2FyzvcqHZVq8XxIc-uX48AgbIIKn6~VcUn4iUG4nYplKNQG7mVm2-2GTs-yJ1ysHGhoxIblfHM-5Dz3StbxB~bSypsKpBHF4W8TulJupYJ-ZGbo9MpbHjLRxcVDUt9cTIWIMtqtjHMCJVjNw6chg4YHlPscni5zH1GWNw6vy39Tu2Es6S2UwePO2GN27coMdNEz0Z4xSCFfYrnk-qpx1pWLwwu~sJn9bZlAvsoorEOJfCvjJc9wjSyKtw6Q1b~~Vz0SeQM7VsjofPUCFFHFaFf4Zi92AOdPYdp5kCwBYWRjE8OBvOTA9XXzGpQ__)

### 5.4. 阶段 4：基于人类反馈的强化学习 (RLHF) - PPO

最后，使用强化学习算法（通常是近端策略优化 - PPO）对 SFT 模型进行微调。在此阶段：

1. SFT 模型（现在是 RL 策略）为给定的提示生成响应。
2. 奖励模型评估此响应并提供一个标量奖励分数。
3. PPO 算法使用此奖励来更新 SFT 模型的权重，鼓励模型生成获得更高奖励的响应。
4. 通常会加入一个惩罚项（例如，与原始 SFT 模型的 KL 散度），以防止 RL 策略在优化奖励时偏离太远，从而避免过度优化和灾难性遗忘。

![基于PPO的RLHF训练流程](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMUludHJ1ZGN0aW9uX3NsaWRlMjhfaW1hZ2Uw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01VbHVkSEoxWkdOMGFXOXVYM05zYVdSbE1qaGZhVzFoWjJVdy5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=MkkG4o2oxn3udTu2qchExu-pSO5Geg5OjuxWloBNrU9PznigUkvCTYjh-KC1r5lDIXbaTtC~HiC56hU38JCmf7uhJDPSJFRw4MQ76dFjq2vdzOD8cER2gO~xudI3Q4g8SUjjRTfrwioXdVP4CF556dygY~oP8cyfmcMhq9rKhOH3l2tWD6zmgoN-aTAFhnhLqK0oNYIdadgr9spI8Bhjyc03yeXxPUozvV4SsAfc5VyvOx3ecJtPgaTvD5dXyHBy5Sc927bCY0Ala55bLHlRvuRMLYN4ajMzGMuNPaiThOEAf92K0553ddY2HAFKXs6QZS4Ib5P-MI~woaf8G47LoA__)

这个多阶段过程，特别是 SFT、RM 和 RLHF 的结合，已成为构建最先进的、与人类对齐的 LLM（如 ChatGPT）的标准方法。PEFT 技术可以在 SFT 和 RLHF 阶段应用，以减少这些步骤的计算需求。

![完整训练流程概览](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930012_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMUludHJ1ZGN0aW9uX3NsaWRlMzBfaW1hZ2Uw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01VbHVkSEoxWkdOMGFXOXVYM05zYVdSbE16QmZhVzFoWjJVdy5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=EHYsrxgCOqc0voDgusyxzqjfcr~MSi4NOyYY7pLhTStfkba4JQumKJivRQugdt6v4o5-iT-0TQv9UbsPeM0vhS1ZJU1N~JU9BhIXKxauU~IMyhIxozboZz~oZ59kjKYZrG3gZPAnKJ2e74XyxXYS3GlCQ4vRLIQVG~BaJXLBd0IRCvTAIt9fwdjHkXDQCxYpHsJ7CsWvryylvxdYuIKrStyLnCx8pOapc2-tJfNhBpTRzi3e43XAytnX8x7yRcKUifYJf2R~kigS0KwYlMle86pTeMbVv-FxlXPDyePwHHxEOJV3Ho9uh1pgbJRfAVJn88hAI9n2o0Bk8gFdJ3OKEQ__)
![训练流程中的模型演变](https://private-us-east-1.manuscdn.com/sessionFile/95YUxBCTQH0UkQS9CT2114/sandbox/Q62kHFunHsagu4YEa51O1T-images_1747286930013_na1fn_L2hvbWUvdWJ1bnR1L2Fzc2V0cy8wMUludHJ1ZGN0aW9uX3NsaWRlMzFfaW1hZ2Uw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvOTVZVXhCQ1RRSDBVa1FTOUNUMjExNC9zYW5kYm94L1E2MmtIRnVuSHNhZ3U0WUVhNTFPMVQtaW1hZ2VzXzE3NDcyODY5MzAwMTNfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyRnpjMlYwY3k4d01VbHVkSEoxWkdOMGFXOXVYM05zYVdSbE16RmZhVzFoWjJVdy5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=rM7-2Jr4N1tBVoWWypsLvs2JMUy5lsgNSR5p2yp-uHKh~S~2KTFwAy14BWh87~rLiYRz7RfToqO1pMkr0sUETiAJ6PTWU-W0oUSso7vFgPgzIKSrjBW8bGHw7uiS8dGfWf3UdsynSvwfvXga5WT-ZU2zanJs26SmZH9uMKGSQHxXz2r~b~iplUBoDR8oDXSsfiEZbDqrpowXs1rPNGkBO4Xgqs3BvlENG8oGb0APdNNmJlJ1BrxDwaihw5hEQM1-~ScsGS~KudDq6SRLqSisZ6DcPeJ1pvq6bjXKTSB-Ka5Chg-gVqCNX8RUiT2VRnBt0jgjIAzzi7DPuIBPxKedQg__)

## 6. 实践案例或案例研究

虽然提供的材料主要侧重于微调的理论方面和各种技术，但这些方法的应用非常广泛。其中一个突出的例子是 LoRA 在图像生成中的应用。

### 6.1. LoRA 用于定制 Stable Diffusion 模型

正如 LoRA 部分所强调的，其最有趣和最实际的应用之一是在图像生成领域，特别是像 Stable Diffusion 这样的模型。Stable Diffusion 模型在初始训练时具有固有的风格。从头开始微调这些模型以实现新的风格或概念可能需要大量资源。

LoRA 提供了一种参数高效的方法来调整 Stable Diffusion，以生成特定艺术风格的图像、描绘特定角色或对象，或遵循某些主题概念。用户可以在代表所需风格或主题的相对较小的数据集上训练 LoRA 适配器。

* **自定义风格：** 艺术家和设计师可以训练 LoRA 适配器，使 Stable Diffusion 具有其独特的艺术特征，或模仿特定的历史艺术运动。
* **角色生成：** LoRA 可用于教模型一致地生成特定角色的图像，这对于讲故事、游戏开发或创建个性化头像非常有用。
* **概念注入：** 可以通过 LoRA 微调将新概念或对象引入模型。

此外，这些训练好的 LoRA 权重通常很小，并且可以轻松共享。甚至可以组合多个 LoRA 适配器并对其效果进行加权，以在生成的图像中产生新颖的混合风格或概念。这种灵活性和效率使 LoRA 成为 AI 艺术社区中的热门选择。

（有关 LoRA 应用于 Stable Diffusion 的视觉示例，请参阅第 4.2.3 节中的图像。）

虽然这是一个具体的例子，但 PEFT 方法通常应用于各种自然语言处理任务，例如专业聊天机器人、自定义内容生成、特定领域的问答以及针对特定行业或数据集的情感分析。

## 7. 工具和库

所提供的演示文稿主要侧重于大型模型微调背后的概念、方法论和算法，而不是详尽列出特定的软件工具或库。然而，大型模型微调领域严重依赖各种开源和专有工具。生态系统中常用的库和框架（尽管在源文档中未作为主要主题详细说明）包括：

* **深度学习框架：** 像 PyTorch 和 TensorFlow 这样的库是构建和训练神经网络（包括大型语言模型）的基础。
* **Transformer 库：** Hugging Face Transformers 是一个被广泛采用的库，提供数千个预训练模型和工具，以便轻松下载、配置和微调它们以适应各种任务。
* **PEFT 库：** 专门为促进参数高效微调 (PEFT) 技术而设计的库，例如 Hugging Face 的 `peft` 库，它实现了 LoRA、前缀微调、提示微调和 IA3 等方法，使得将这些技术应用于 Transformer 模型更加容易。
* **分布式训练库：** 为了处理大型模型的规模，通常使用 PyTorch Distributed、TensorFlow Distributed、DeepSpeed 和 Horovod 等工具将训练分布到多个 GPU 和节点上。
* **实验跟踪：** 像 Weights & Biases、TensorBoard 和 MLflow 这样的工具对于记录指标、可视化训练进度和管理实验至关重要。
* **数据处理库：** 诸如 Pandas、NumPy 和 Datasets (来自 Hugging Face) 之类的库对于准备和管理微调中使用的数据集至关重要。

工具的具体选择通常取决于模型架构、所采用的微调技术、可用的基础设施以及团队的专业知识。源材料中的算法讨论（例如，用于 RLHF 的 PPO，提及用于图像生成的 Stable Diffusion）意味着使用此类底层框架和库来实现和试验这些先进技术。

## 8. 最佳实践与注意事项

微调大型模型虽然功能强大，但需要仔细规划和执行。以下是一些需要牢记的最佳实践和注意事项：

* **理解您的任务和数据：** 清晰定义您要解决的问题，并彻底分析您的特定任务数据集。微调数据的质量、数量和相关性对成功至关重要。“垃圾进，垃圾出”的原则依然适用。
* **选择合适的预训练模型：** 选择一个非常适合您目标任务的基础预训练模型。考虑其架构、大小、预训练数据以及在类似任务上的任何现有基准。
* **选择适当的微调策略：**
  * **全参数微调 (FPFT)：** 如果您有足够的数据和计算资源，可以考虑此方法。它可以提供最高的性能，但成本高昂，并且存在灾难性遗忘的风险。
  * **参数高效微调 (PEFT)：** 对于大多数情况，尤其是在资源有限或需要快速迭代的情况下，强烈建议使用 PEFT 方法（LoRA、Adapters、Prompt Tuning、IA3 等）。它们显著减少了可训练参数、训练时间和存储需求。
  * 根据性能、参数效率和针对您特定用例的实施简易性等方面的权衡，评估不同的 PEFT 方法。
* **数据准备和预处理：** 确保您的微调数据干净、格式正确，并且能够代表您在推理时期望的分布。对于指令微调，制作高质量的指令和响应至关重要。
* **超参数调整：** 试验关键超参数，例如学习率、批量大小、周期数以及所选 PEFT 方法特有的任何参数（例如 LoRA 中的秩 `r`、适配器瓶颈维度）。使用验证集指导超参数选择。
* **评估指标：** 定义清晰且相关的评估指标，以准确反映目标任务的成功。在训练期间和在保留测试集上密切监控这些指标。
* **防止过拟合：** 较小的微调数据集可能导致过拟合。采用正则化技术，根据验证性能进行提前停止，并在适当时考虑数据增强。
* **灾难性遗忘：** 在进行全参数微调时，请注意灾难性遗忘，即模型丢失其部分通用预训练知识。使用较低的学习率、逐步解冻层或重放一些预训练数据等技术有时可以帮助缓解此问题。
* **计算资源：** 评估您可用的计算预算（GPU、内存）。这将严重影响您对模型大小和微调技术的选择。
* **迭代方法：** 微调通常是一个迭代过程。从更简单的方法或更小的模型开始，快速迭代，分析结果，并根据需要逐渐增加复杂性或规模。
* **道德考量与偏见：** 请注意，预训练模型可能会从其训练数据中继承偏见。在特定数据集上进行微调可能会放大或引入新的偏见。主动审核模型的公平性，并在可能的情况下减轻偏见。
* **可复现性：** 跟踪您的实验，包括模型版本、数据集、超参数和代码，以确保可复现性。
* **推理效率：** 考虑微调模型的推理延迟和成本，特别是对于实时应用。一些 PEFT 方法（如合并后的 LoRA）不会增加推理开销，而其他方法（如未合并的适配器）可能会增加。

通过仔细考虑这些要点，您可以增加成功微调大型模型以实现预期结果的可能性。

## 9. 结论

大型模型微调已成为最先进人工智能模型实际应用中的一项关键技术。它提供了一条强大且相对高效的途径，可以将拥有海量学习知识的预训练模型调整到特定的任务、领域和用户偏好。通过理解基本概念、全参数方法与参数高效方法之间的区别，以及各种 PEFT 方法（如 LoRA、Adapters、Prefix/Prompt Tuning 和 IA3）的细微差别，从业者可以做出明智的决策，从而有效地利用这些模型。

从通用预训练模型到专业化、高性能且良好对齐的 AI 助手的过程，需要仔细考虑数据、计算资源、评估策略和道德影响。虽然全参数微调具有深度适应的潜力，但 PEFT 方法已经使大型模型定制大众化，从而以大幅降低的成本和技术壁垒实现了显著的进步。

随着研究的不断发展，我们可以期待出现更复杂、更高效的微调技术，进一步增强模型性能，减少资源消耗，并提高我们创建不仅智能而且安全、可靠且符合人类价值观的 AI 系统的能力。有效微调的能力对于任何希望充分利用大规模人工智能潜力的人来说，仍将是一项关键技能。

## 10. 进一步的资源

本指南中讨论的概念和技术源于一个快速发展的研究领域。为了更深入地理解并保持更新，请考虑探索以下资源（其中一些在源材料中被引用）：

* **研究论文：**
  * *Parameter-Efficient Transfer Learning for NLP* (Houlsby et al., 2019) - 介绍了 Adapter 微调。
  * *LoRA: Low-Rank Adaptation of Large Language Models* (Hu et al., 2021) - 介绍了 LoRA。
  * *Prefix-Tuning: Optimizing Continuous Prompts for Generation* (Li & Liang, 2021) - 介绍了 Prefix Tuning。
  * *GPT Understands, Too* (Liu et al., 2021) - 介绍了 P-Tuning。
  * *The Power of Scale for Parameter-Efficient Prompt Tuning* (Lester et al., 2021) - 探讨了 Prompt Tuning。
  * *Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning* (Aghajanyan et al., 2020)
  * *Learning to Summarize from Human Feedback* (Stiennon et al., 2020) - 详细介绍了用于摘要的 RLHF。
  * *Fine-Tuning Language Models from Human Preferences* (Ziegler et al., 2019) - 关于 LLM 的 PPO 和奖励学习的早期工作。
  * *Proximal Policy Optimization Algorithms* (Schulman et al., 2017) - 介绍了 PPO。
  * *Asynchronous Methods for Deep Reinforcement Learning* (Mnih et al., 2016)
* **博客文章和文章：**
  * Sebastian Raschka 的博客 (magazine.sebastianraschka.com) 经常发表关于 LLM 训练和 RLHF 的文章。
  * 诸如 `mercity.ai`、`medium.com`（多位作者）等网站上的文章和教程，以及 Hugging Face 或深度学习框架提供商的官方文档。
* **在线社区和存储库：**
  * Hugging Face Transformers、PEFT、DeepSpeed 等库的 GitHub 存储库。
  * 专注于 NLP 和大型语言模型的论坛和讨论组。

这份清单并非详尽无遗，因为新的研究和资源层出不穷。

## 11. 致谢

这份关于大型模型微调的教育指南是根据用户提供的以下三份文档中呈现的信息和视觉材料汇编和综合而成的：

1. `01Intrudction.pptx`
2. `02Transfer.pptx`
3. `03PTuning.pptx`

其结构、关键概念、技术细节和说明性示例主要源自这些来源。目的是以清晰、全面且易于访问的 Markdown 格式组织和呈现这些信息，以便在 GitHub 上用于教育目的。
